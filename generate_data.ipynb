{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_E2C61e0n7h5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "import torch\n",
        "\n",
        "from torchvision import datasets\n",
        "\n",
        "from torchvision.datasets import Imagenette\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torchvision.transforms.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h-JE1j-v39u",
        "outputId": "a2bf1ce4-df33-4430-80b0-c9e87f031a7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"/content/drive/MyDrive/Colab_Notebooks/ViTAR/Dataset\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6ZINnIIwRy1",
        "outputId": "382f8963-423b-42a9-d8f9-4eef7cbc192e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/ViTAR/Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "train_data = Imagenette(root='data', split='train', download=True, transform=train_transforms)\n",
        "test_val_data = Imagenette(root='data', split='val', download=False, transform=test_transforms)\n",
        "\n",
        "test_data, val_data = torch.utils.data.random_split(test_val_data, [int(0.5 * len(test_val_data)) + len(test_val_data) % 2, int(0.5 * len(test_val_data))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzybVEYVoKok",
        "outputId": "4877b5ec-c234-46c7-bc79-ee81b19320b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz to data/imagenette2.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.56G/1.56G [00:35<00:00, 43.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/imagenette2.tgz to data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_binned_dataset(dataset, bins, output_dir):\n",
        "    \"\"\"\n",
        "    Perform a single pass through the dataset to preprocess and save binned datasets.\n",
        "\n",
        "    Args:\n",
        "        dataset: A PyTorch dataset (e.g., ImageFolder).\n",
        "        bins: List of resolution bins to categorize the images.\n",
        "        output_dir: Path to save the dataset organized by bins.\n",
        "\n",
        "    Returns:\n",
        "        None. The function saves resized images and labels to disk.\n",
        "    \"\"\"\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Create directories for each bin\n",
        "    bin_dirs = {bin_size: os.path.join(output_dir, f\"bin_{bin_size}\") for bin_size in bins}\n",
        "    for bin_dir in bin_dirs.values():\n",
        "        os.makedirs(bin_dir, exist_ok=True)\n",
        "\n",
        "    # Function to find the closest bin\n",
        "    def find_closest_bin(resolution, bins):\n",
        "        return min(bins, key=lambda x: abs(x - resolution))\n",
        "\n",
        "    # Single pass through the dataset\n",
        "    for idx in tqdm(range(len(dataset))):\n",
        "        image, label = dataset[idx]\n",
        "\n",
        "        # Ensure the image is a PIL.Image for resizing\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            image = F.to_pil_image(image)\n",
        "\n",
        "        resolution = max(image.size[1], image.size[0])  # Width, Height for PIL.Image\n",
        "        closest_bin = find_closest_bin(resolution, bins)\n",
        "\n",
        "        # Resize the image to the bin's resolution\n",
        "        transform_resize = transforms.Compose([\n",
        "            transforms.Resize((closest_bin, closest_bin)),  # Resize to square\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        image_resized = transform_resize(image)\n",
        "\n",
        "        # Save the resized image\n",
        "        bin_dir = bin_dirs[closest_bin]\n",
        "        image_path = os.path.join(bin_dir, f\"{idx}.png\")\n",
        "        image_resized_pil = transforms.ToPILImage()(image_resized)\n",
        "        image_resized_pil.save(image_path)\n",
        "\n",
        "        # Save the label\n",
        "        with open(os.path.join(bin_dir, \"labels.txt\"), \"a\") as label_file:\n",
        "            label_file.write(f\"{idx}.png {label}\\n\")\n",
        "\n",
        "    print(\"Binning and saving completed.\")\n"
      ],
      "metadata": {
        "id": "XXOCDyNZvlee"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "# Define bins\n",
        "bins = [224, 448, 640, 896, 1280, 1920, 2800, 4032]\n",
        "\n",
        "\n",
        "# Perform single-pass binning and saving train\n",
        "output_dir = \"./binned_dataset/train\"\n",
        "save_binned_dataset(train_data, bins, output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB1gl9_YoTXK",
        "outputId": "bf1d24e7-1c51-48b3-cd11-3617f78574d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 332/9469 [01:04<16:35,  9.18it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform single-pass binning and saving val\n",
        "output_dir = \"./binned_dataset/val\"\n",
        "save_binned_dataset(val_data, bins, output_dir)"
      ],
      "metadata": {
        "id": "zSALOzTCxgha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform single-pass binning and saving test\n",
        "output_dir = \"./binned_dataset/test\"\n",
        "save_binned_dataset(test_data, bins, output_dir)"
      ],
      "metadata": {
        "id": "RRIBqwdixhvw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}