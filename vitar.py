# -*- coding: utf-8 -*-
"""ViTAR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BL8IpwxWlel3dhCrB1inpzBLygw7JnOl
"""

from torch.utils.data import DataLoader
import torch
import torch.nn as nn
from torchvision import datasets

from torchvision.datasets import Imagenette
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import torch.nn.functional as F

def multi_res_collate_fn(batch):
    images, labels = zip(*batch)

    # Process images as a list instead of stacking into a batch
    images = [img for img in images]
    labels = torch.tensor(labels)

    return images, labels

mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]

train_transforms = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize(mean=mean, std=std)])

test_transforms = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=mean, std=std)])

train_data = Imagenette(root='data', split='train', download=True, transform=train_transforms)
test_val_data = Imagenette(root='data', split='val', download=False, transform=test_transforms)

test_data, val_data = torch.utils.data.random_split(test_val_data, [int(0.5 * len(test_val_data)) + len(test_val_data) % 2, int(0.5 * len(test_val_data))])

batch_size = 32
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=multi_res_collate_fn)
val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=multi_res_collate_fn)
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=multi_res_collate_fn)

len(test_val_data)

def calculate_padding(img_dim, patch_dim):
    H = img_dim
    H_pad = [0, 0]
    if H % patch_dim != 0:
        if H % 2 == 1:
            H_pad[0] += 1
        H_pad[0] += (patch_dim - H % patch_dim) // 2
        H_pad[1] += (patch_dim - H % patch_dim) // 2
    return H_pad

class PatchEmbed(nn.Module):
    def __init__(self,patch_size=16,in_chans=3,embed_dim=768):
        super().__init__()
        self.patch_size=patch_size
        self.embedding_layer = nn.Conv2d(in_chans,embed_dim,kernel_size=patch_size,stride=patch_size, padding=0)

    def forward(self,x):
        img_size = x.shape[-2:]
        padding_height = calculate_padding(img_size[0], self.patch_size)
        padding_width = calculate_padding(img_size[1], self.patch_size)
        padding = (padding_height, padding_width)
        x = F.pad(x, (padding[1][0], padding[1][1], padding[0][0], padding[0][1]))
        x = self.embedding_layer(x)
        return x

def grid_pad(img, Gh, Gw):
    B, C, H, W = img.shape

    if H > 2 * Gh:
        if H % 2 != 0:
            h_pad = 1
        else:
            h_pad = 0
    else:
        h_pad = 2 * Gh - H

    if W > 2 * Gw:
        if W % 2 != 0:
            w_pad = 1
        else:
            w_pad = 0
    else:
        w_pad = 2 * Gw - W

    new_H, new_W = H + h_pad, W + w_pad

    if h_pad == 0 and w_pad == 0:
        return img

    padded_tensor = torch.zeros(B, C, new_H, new_W)

    if h_pad < H and w_pad < W:
        padded_tensor[:, :, :2 * h_pad:2, :2 * w_pad:2] = img[:, :, :h_pad, :w_pad]
        padded_tensor[:, :, 2 * h_pad:, :2 * w_pad:2] = img[:, :, h_pad:, :w_pad]
        padded_tensor[:, :, :2 * h_pad:2, 2 * w_pad:] = img[:, :, :h_pad, w_pad:]
        padded_tensor[:, :, 2 * h_pad:, 2 * w_pad:] = img[:, :, h_pad:, w_pad:]
    elif h_pad < H:
        padded_tensor[:, :, :2 * h_pad:2, :2 * W:2] = img[:, :, :h_pad, :]
        padded_tensor[:, :, 2 * h_pad:, :2 * W:2] = img[:, :, h_pad:, :]
    elif w_pad < W:
        padded_tensor[:, :, :2 * H:2, :2 * w_pad:2] = img[:, :, :, :w_pad]
        padded_tensor[:, :, :2 * H:2, 2 * w_pad:] = img[:, :, :, w_pad:]
    else:
        padded_tensor[:, :, :2 * H:2, :2 * W:2] = img[:, :, :, :]

    return padded_tensor

class FeedForward(nn.Module):
    def __init__(self, embed_dim, hidden_dim=None, dropout=0.1):
        """
        FeedForward Network for Adaptive Token Merging (ATM).

        Args:
            embed_dim (int): Dimension of the input and output embeddings (D).
            hidden_dim (int, optional): Dimension of the intermediate hidden layer. Defaults to 4 * embed_dim.
            dropout (float): Dropout rate. Defaults to 0.1.
        """
        super().__init__()
        if hidden_dim is None:
            hidden_dim = embed_dim * 4  # Default to 4x expansion.

        self.fc1 = nn.Linear(embed_dim, hidden_dim)  # First linear layer
        self.activation = nn.GELU()  # Non-linear activation
        self.fc2 = nn.Linear(hidden_dim, embed_dim)  # Second linear layer
        self.dropout = nn.Dropout(dropout)  # Dropout for regularization

    def forward(self, x):
        """
        Forward pass of the FFN.

        Args:
            x (torch.Tensor): Input tensor of shape (B, N, D).

        Returns:
            torch.Tensor: Output tensor of shape (B, N, D).
        """
        x = self.fc1(x)  # (B, N, hidden_dim)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.fc2(x)  # (B, N, embed_dim)
        x = self.dropout(x)
        return x

class AdaptiveTokenMerger(nn.Module):
    def __init__(self, adaptive_ratio=2, Gh=14, Gt=14):
        super().__init__()
        self.adaptive_ratio = adaptive_ratio
        self.Gh = Gh
        self.Gt = Gt
        self.pooling = nn.AvgPool2d(kernel_size=adaptive_ratio, stride=adaptive_ratio)
        self.attention = nn.MultiheadAttention(embed_dim=768, num_heads=12, batch_first=True)
        self.ffn = FeedForward(embed_dim=768)

    def ae_iteration(self, x):
        B, D, H, W = x.shape
        y = grid_pad(x, self.Gh, self.Gt)
        y = self.pooling(y)
        B_, D_, H_, W_ = y.shape
        x = x.reshape(B, -1, D)
        y = y.reshape(B, -1, D)

        atm_out, _ = self.attention(y, x, x)
        atm_out = y + atm_out
        atm_out = self.ffn(atm_out)
        atm_out = atm_out.reshape(B_, H_, W_, D_)
        return atm_out.permute(0, 3, 1, 2)

    def forward(self, x):
        B, E, H, W = x.shape
        while H > self.Gh or W > self.Gt:
            x = self.ae_iteration(x)
            B, E, H, W = x.shape
        return x

def interpolate_positional_encoding(positional_encoding, perturbed_coords):
    """
    Interpolates a 3D positional encoding tensor (D, H, W) at perturbed spatial coordinates.

    Args:
        positional_encoding (torch.Tensor): A tensor of shape (D, H, W) where:
                                             - D: Embedding dimension
                                             - H, W: Spatial grid dimensions
        perturbed_coords (torch.Tensor): A tensor of perturbed coordinates of shape (N, 2),
                                         where each row is a pair of (x, y) spatial coordinates
                                         in the range [-0.5, 0.5].

    Returns:
        torch.Tensor: Interpolated positional encodings at the perturbed coordinates, of shape (N, D).
    """
    # Get the dimensions of the positional encoding
    D, H, W = positional_encoding.shape

    # Reshape positional_encoding to (1, D, H, W) for grid_sample
    positional_encoding = positional_encoding.unsqueeze(0)  # Shape: (1, D, H, W)

    # Reshape perturbed_coords to match grid_sample input
    # Expected shape: (1, N, 1, 2), where N is the number of perturbed coordinates
    perturbed_coords = perturbed_coords.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, N, 2)

    # Perform grid sampling
    interpolated = F.grid_sample(
        positional_encoding,  # Shape: (1, D, H, W)
        perturbed_coords,     # Shape: (1, 1, N, 2)
        mode="bilinear",
        align_corners=True
    )

    # Reshape to grid
    return interpolated.squeeze().reshape(D, H, W)

class FuzzyPositionalEncoding(nn.Module):
    def __init__(self, H, W, D):
        super().__init__()
        self.H = H
        self.W = W
        self.D = D
        self.inference = False
        self.pos_encoding = nn.Parameter(torch.zeros(D, H, W))

    def perturb(self):
        perturbation_x = torch.rand(self.H, self.W) - 0.5  # Shape: (H, W)
        perturbation_y = torch.rand(self.H, self.W) - 0.5  # Shape: (H, W)
        perturbation_coords = torch.stack([perturbation_x.flatten(), perturbation_y.flatten()], dim=-1)  # Shape: (H*W, 2)
        perturbed_pos_encoding = interpolate_positional_encoding(self.pos_encoding, perturbation_coords)
        return perturbed_pos_encoding


    def forward(self, x):
        B, E, H, W = x.shape
        if not self.inference:
            pos_encoding = self.perturb()
        else:
            pos_encoding = self.pos_encoding.detach()
        resized_pos_encoding = F.interpolate(pos_encoding.unsqueeze(0), size=(H, W), mode="bilinear", align_corners=True)
        return x + resized_pos_encoding.squeeze()


    def train(self, mode=True):
        """
        Overrides the train method to ensure that the FPE layer enters training mode
        when the parent model switches to training mode.
        """
        self.inference = not mode  # In training mode, enable perturbations
        return super().train(mode)

    def eval(self):
        """
        Overrides the eval method to ensure that the FPE layer enters inference mode
        when the parent model switches to evaluation mode.
        """
        self.inference = True  # In evaluation mode, disable perturbations
        return super().eval()

class ViTAR(nn.Module):
    def __init__(self, embedding_dim=768, hidden_dim=4 * 768, Gh=14, Gt=14, pe_H=64, pe_W=64, num_heads=12, num_layers=12, num_classes=10):
        super().__init__()
        self.embedding_layer = PatchEmbed(patch_size=16, embed_dim=embedding_dim)
        self.pe = FuzzyPositionalEncoding(pe_H, pe_W, embedding_dim)
        self.atm = AdaptiveTokenMerger(Gh=Gh, Gt=Gt)
        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=hidden_dim, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(embedding_dim, num_classes)
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embedding_dim))

    def forward(self, x):
        x = self.embedding_layer(x)
        x = self.pe(x)
        x = self.atm(x)
        B, E, H, W = x.shape
        x = x.reshape(B, -1, E)
        cls_token = self.cls_token.repeat(B, 1, 1)
        x = torch.cat([cls_token, x], dim=1)
        x = self.transformer_encoder(x)
        out = self.fc(x[:, 0, :])
        return out